#### AWS EC2, Docker, CICD (Github Actions), React, Javascript, Node, Express, MongoDB (Atlas), CRUD Ops

- https://www.youtube.com/watch?v=WwxSNIrW8bk

#### Simple Deployment Steps

- Launch an instance
- Install Docker and Git on it
  - `curl https://get.docker.com | bash`
- git clone https://github.com/idesta/product_store.git
- cd product_store/
- `ls` -> to check the docker file existed on it
- `docker build -t prd_str_cicd .`
- `docker images`
  - to list the build image
- `docker run -d --name prd_str_cicd_demo -p 5000:5000 prd_str_cicd`
  - to list the running containers
    - `docker ps` or `docker ps -a`
    - If there is no up containers folloe the next steps
      - Inside the product_store folder
        - `touch .env`
        - Copy and paste the `mongo url` and `port` from the local repo
        - Remove the running container if exists
          - `docker rm prd_str_cicd_demo`
        - Run by passing the `.env` manually
          - ``docker run -d --name prd_str_cicd_demo --env-file .env -p 5000:5000 prd_str_cicd
- Check on your browser
  - Instance_Public_IP:5000
  - Test the functionality

- On your local repo
  - `mkdir -p .github/workflows` or simply create `.github` folder and inside of it create `workflows` directory.
  - Create and put `ci_cd.yml` file on the .github/workflows directory

- Put the instance username (ubuntu) on the docker group
  - Be the super user (root) `sudo su`
  - `usermod -aG docker ubuntu`
  - Exit from the root and try to list the docker images
    - If not works disconnect and reconnect again

- Make sure the secrets are correct and update something new and the docker hub token should have read and write permission
- Remove the container from the Instance, Make an update and push to github and see the details

- If you terminate the EC2 instance and want to test back
  - Just go to your repo
    - click `Settings`
      - Click `Secrets and Variables`
      - Click `Actions`
      - Click `EC2_HOST` and update the new IP Address.

# The New Deployment Architecture

.
├── ansible
│ ├── inventory
│ │ └── hosts.ini # Auto-generated by Terraform
│ └── playbooks
│ ├── site.yml # The "Master" switch
│ ├── base.yml # System prep (All nodes)
│ ├── master.yml # Control Plane setup (Master only)
│ ├── worker.yml # Join cluster (Workers only)
│ └── cni.yml # Networking (Master only)
├── terraform
│ ├── main.tf # Defines 1 Master + 2 Workers
│ ├── ansible.tf # Writes the hosts.ini file
│ ├── security-group.tf # Opens K8s ports
│ ├── variables.tf # Configuration vars
│ ├── outputs.tf # Displays IPs
│ ├── terraform.tfvars # Your secret values (NEW)
│ └── provider.tf # AWS Connection
└── product-node-ci-cd-2.pem # Your SSH key

k8s arch
k8s/
├── namespaces/
│ └── app-namespace.yaml # Defines 'production' namespace
├── secrets/
│ └── atlas-secret.yaml # Encrypted Atlas Connection String
├── backend/
│ ├── deployment.yaml # Backend logic (2 replicas)
│ └── service.yaml # Internal ClusterIP service
├── frontend/
│ ├── deployment.yaml # Frontend logic (2 replicas)
│ └── service.yaml # Internal ClusterIP service
├── ingress/
│ └── app-ingress.yaml # External access (Nginx Ingress)
└── kustomization.yaml # The "Glue" that links everything

## k8s master admin.conf to local kube file

- Get the new config: Run this on the Master node to see your new credentials:
  - `sudo cat /etc/kubernetes/admin.conf`
- Update your Laptop: Copy that text, then on your local laptop, open your config file:
  - `nano ~/.kube/config`
- Change the IP: Inside that file on your laptop, look for the line server:
  - `https://172.31.x.x:6443. Change 172.31.x.x to the Public IP of your new Master node.`

- Find the cluster:
  - server: https://54.226.140.252:6443
    - section and add the `insecure-skip-tls-verify: true` line. It should look like this:
    - insecure-skip-tls-verify: true # <--- Add this line
    - # certificate-authority-data: ... (You can comment this line out or delete it)

- Move to your project root
  - cd ~/Documents/Github/product_store/

- `kubectl delete -k k8s/`

- Apply everything at once
  - `kubectl apply -k k8s/`

## Finding the public URL

- Quick Step: Install the Nginx Ingress Controller
  Run this command from your laptop to install the official Nginx Ingress Controller in your cluster:

- `kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/baremetal/deploy.yaml`

- Finding your Public URL
  - `kubectl get svc -n ingress-nginx`
  - Adding that port to the AWS Security Group

## ArgoCD installation

- `kubectl create namespace argocd`
- `kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml`

- Access the ArgoCD UI: By default, the UI is not public. We will use a NodePort to see it (just like we did for your app).
  - `kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "NodePort"}}'`
  - `kubectl get svc -n argocd`

- Look for argocd-server. It will have a port like 80:3XXXX. You can now open https://<MASTER_PUBLIC_IP>:3XXXX in your browser. (Ignore the SSL certificate warning).

- Add the port to Security group
